{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks with PyTorch\n",
    "\n",
    "Next I'll show you how to build a neural network with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things like usual\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-13bcd9cf5dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 128, 64, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0302, -0.0043,  0.0138,  ...,  0.0130, -0.0327,  0.0011],\n",
      "        [-0.0293, -0.0159,  0.0004,  ...,  0.0035, -0.0039,  0.0201],\n",
      "        [-0.0330,  0.0340,  0.0208,  ..., -0.0017, -0.0288,  0.0073],\n",
      "        ...,\n",
      "        [-0.0074, -0.0238, -0.0057,  ...,  0.0241, -0.0118, -0.0257],\n",
      "        [ 0.0230, -0.0086, -0.0228,  ...,  0.0054,  0.0199,  0.0059],\n",
      "        [-0.0172,  0.0185,  0.0108,  ...,  0.0175,  0.0260,  0.0208]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0124, -0.0288, -0.0025, -0.0147, -0.0105, -0.0244, -0.0025,  0.0137,\n",
      "        -0.0149, -0.0005, -0.0217, -0.0222,  0.0355,  0.0112,  0.0344,  0.0139,\n",
      "         0.0092,  0.0011, -0.0071,  0.0330,  0.0128, -0.0130,  0.0191, -0.0053,\n",
      "         0.0058,  0.0322,  0.0112,  0.0003, -0.0070,  0.0094, -0.0155,  0.0287,\n",
      "        -0.0234,  0.0262,  0.0044,  0.0124,  0.0119,  0.0065, -0.0116, -0.0095,\n",
      "         0.0134, -0.0023, -0.0230,  0.0072,  0.0173, -0.0064, -0.0190, -0.0188,\n",
      "         0.0188,  0.0059, -0.0357, -0.0016, -0.0292, -0.0237, -0.0332, -0.0084,\n",
      "         0.0355, -0.0307, -0.0123,  0.0282, -0.0293, -0.0188, -0.0318,  0.0252,\n",
      "         0.0036,  0.0193,  0.0190,  0.0110, -0.0052, -0.0308, -0.0060, -0.0008,\n",
      "         0.0011, -0.0025,  0.0299,  0.0255, -0.0124, -0.0314, -0.0279, -0.0104,\n",
      "        -0.0051,  0.0130, -0.0221, -0.0118,  0.0257, -0.0277,  0.0123,  0.0255,\n",
      "         0.0243,  0.0136, -0.0087, -0.0281,  0.0343,  0.0072,  0.0327,  0.0283,\n",
      "        -0.0044, -0.0189,  0.0288,  0.0314,  0.0196, -0.0275, -0.0104,  0.0083,\n",
      "         0.0011, -0.0209, -0.0221,  0.0028, -0.0258,  0.0320, -0.0173, -0.0332,\n",
      "        -0.0283, -0.0340, -0.0141,  0.0057,  0.0324,  0.0236, -0.0334, -0.0015,\n",
      "         0.0099, -0.0286, -0.0260, -0.0198, -0.0041,  0.0301, -0.0069, -0.0077],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0161, -0.0023, -0.0020,  ...,  0.0128,  0.0073,  0.0022],\n",
       "        [-0.0270,  0.0016,  0.0164,  ..., -0.0058,  0.0028,  0.0157],\n",
       "        [ 0.0276,  0.0017,  0.0188,  ...,  0.0079, -0.0029,  0.0012],\n",
       "        ...,\n",
       "        [-0.0012, -0.0276, -0.0028,  ..., -0.0071,  0.0125, -0.0041],\n",
       "        [ 0.0165,  0.0027,  0.0094,  ...,  0.0028, -0.0019,  0.0231],\n",
       "        [-0.0049, -0.0066,  0.0018,  ...,  0.0109, -0.0071,  0.0114]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.\n",
    "\n",
    ">**Exercise:** Build a network to classify the MNIST images with _three_ hidden layers. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell with your model to make sure it works ##\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
